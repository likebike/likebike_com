<!DOCTYPE html>
<!--[if lt IE 7]>  <html class="no-js ie ie6 ie-lte9 ie-lte8 ie-lte7 ie-lte6" lang=en> <![endif]-->
<!--[if IE 7]>     <html class="no-js ie ie7 ie-lte9 ie-lte8 ie-lte7" lang=en> <![endif]-->
<!--[if IE 8]>     <html class="no-js ie ie8 ie-lte9 ie-lte8" lang=en> <![endif]-->
<!--[if IE 9]>     <html class="no-js ie ie9 ie-lte9" lang=en> <![endif]-->
<!--[if gt IE 9]>  <html class="no-js ie ie-gt9" lang=en> <![endif]-->
<!--[if !IE]><!--> <html class="no-js not-ie" lang=en> <!--<![endif]-->
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link rel="shortcut icon" type="image/vnd.microsoft.icon" href="/favicon.ico">
    <title>How to write fast Rust code</title>
    <link rel="stylesheet" type="text/css" href="/static/lib/yui-3.12.0/reset_base_fonts-min.css">
    <link rel="stylesheet" type="text/css" href="/static/lib/font-awesome-4.5.0/css/font-awesome.css">
    <link rel="stylesheet" type="text/css" href="/static/css/github-markdown.css?_=1569532993.1">
    <link rel="stylesheet" type="text/css" href="/static/css/base_pretty.css?_=1569537471.13">
    
    <script type="text/javascript" src="/static/lib/underscore-1.5.2/underscore.js"></script>
    <script type="text/javascript" src="/static/lib/jquery-1.10.2/jquery.js"></script>
    <script type="text/javascript">
      jQuery.noConflict();
    </script>
    <script type="text/javascript" src="/static/js/base_pretty.js?_=1569532676.98"></script>
    
  </head>
  <body>
    <section id=header>
      <a id=logo href="/index.html"><div class="asciiLogo vcompressed">
  -------  ___@
-------   `\  L,
  ----  (*)/  (*)
</div></a>
      <div id=slogan>Biking is Freedom</div>
      <hr>
    </section>
    <section id=content>
      

<h1 class=postTitle>DOLLAR OPEN title CLOSE</h1>
<div class=postMeta>Published <span class=postDate>DOLLAR OPEN date CLOSE</span>,&nbsp; by <span class=postAuthor>DOLLAR OPEN meta['author'] CLOSE</span></div>
<img class="postMainImg roundBorder" src="DOLLAR OPEN image CLOSE" />
<article class="markdown-body"><h1>How To Write Fast Rust Code</h1>
<h2><a id=the-journey href="#the-journey">The Journey from <code>eval</code> to <code>fasteval</code></a></h2>
<p>I did a line-for-line port my <code>eval</code> library from Go to Rust, and right away it was <strong>5x</strong> faster; I was pretty happy.&nbsp; But when I tried to further improve performance using techniques from other languages, it got slower...&nbsp; and the harder I tried, the slower it got!&nbsp; Rust performance was <em>not</em> intuitive to me.</p>
<p>Finally, after learning <em>why</em> my code was slow, I was able to boost performance <strong>12000x</strong>, and my library was worthy of a new name: <a href="https://github.com/likebike/fasteval"><code>fasteval</code></a>.</p>
<p><a href="https://github.com/likebike/fasteval#performance-benchmarks"><img alt="fasteval Performance" src="https://raw.githubusercontent.com/likebike/fasteval/master/benches/results/20191225/fasteval-compiled.png"></a></p>
<p>Here is a log chart showing <code>fasteval</code>'s performance compared to other similar libraries.&nbsp; <code>fasteval</code> is represented by the little blue columns, and as you can see, it is <em>significantly</em> faster.</p>
<p>Rust performance makes sense to me now.&nbsp; Here are the lessons I learned.</p>
<h2><a id=how-to-measure href="#how-to-measure">Basic Skill #1: How To Take Noise-Free Measurements</a></h2>
<p>The first step to improving performance is to measure, measure, measure... but these measurements will be affected by <a href="https://easyperf.net/blog/2019/08/02/Perf-measurement-environment-on-Linux">many variables</a>.&nbsp; I try to eliminate three of them: Background Applications, Power Management, and Binary Layout.</p>
<h3><a id=bg-apps href="#bg-apps">Background Applications</a></h3>
<p>This one's easy: close all the background apps, <em>especially</em> web browsers which constantly consume cycles from all your CPU cores.</p>
<h3><a id=power-mgt href="#power-mgt">CPU Power Management</a></h3>
<p>Here is how I disable power-saving mode on Ubuntu 18.04:</p>
<pre><code class="bash">for F in /sys/devices/system/cpu/cpufreq/policy*/scaling_governor; do echo performance &gt;$F; done
</code></pre>

<h3><a id=layout-rand href="#layout-rand">Layout Randomization</a></h3>
<p>The compiler often makes poor decisions about the placement of your code within the binary, and your performance suffers.&nbsp; To mitigate this, I use a Layout Randomization technique similar to <a href="https://www.youtube.com/watch?v=r-TLSBdHe1A">Coz</a>: during each iteration of my benchmark loop, I inject a random number of no-op instructions into my benchmark code (using <code>sed</code>).&nbsp; This shifts everything around in the address space so that I end up hitting all fast and slow scenarios.&nbsp; I then run the benchmark loop many times, until I no longer observe any performance improvements for 500 seconds.&nbsp; At that point, I say that I have reached a stable point and can draw conclusions from the statistics.</p>
<p>I define this macro in my benchmark code:</p>
<pre><code class="rust">#![feature(test)]
extern crate test;
use test::{Bencher, black_box};

macro_rules! memshift {
    () =&gt; { 
        {
            let x = black_box(0);
            let x = black_box(x+1);

            //SHIFT_CODE

            black_box(x);  // Silence 'unused variable' warning.
        }
    }
}
</code></pre>

<p>I then call <code>memshift!();</code> at the beginning of my benchmark functions.</p>
<p>Here is my benchmark loop, which performs Layout Randomization:</p>
<pre><code class="bash">while true; do
    echo &quot;time: $(date +%s)&quot;;
    cat bench.rs.tmpl | sed &quot;s|//SHIFT_CODE|$( N=$(( 1 + $RANDOM % 1024 ));
                                               while [[ $N &gt; 0 ]]; do
                                                   N=$(( $N - 1 ));
                                                   echo -n 'let x=black_box(x+1);';
                                               done )|g&quot; &gt;bench.rs;
    RUSTFLAGS=&quot;--emit=asm&quot; cargo bench;
done &gt;bench.out
</code></pre>

<p>I monitor the results with this:</p>
<pre><code class="bash">cat bench.out | awk -v &quot;now=$(date +%s)&quot; '
    $1==&quot;time:&quot;{when=$2}
    $3==&quot;...&quot; &amp;&amp; $4==&quot;bench:&quot; {
        gsub(/,/, &quot;&quot;, $5);
        v=$5+0;
        counts[$2]+=1; sums[$2]+=v;
        if (mins[$2]==&quot;&quot; || v&lt;mins[$2]) {
            mins[$2]=v; w[$2]=when;
        }
    }
    END{
        printf &quot;%-40s %9s %9s %16s\n&quot;, &quot;000_NAME&quot;, &quot;MEAN&quot;, &quot;MIN&quot;, &quot;WHEN&quot;;
        for (k in mins) {
            printf &quot;%-40s %9d %9d ns/iter    %5ds ago\n&quot;,k,sums[k]/counts[k],mins[k],now-w[k];
        }
    }
' | sort
</code></pre>

<p>I use the 'minimum' times as my final result.&nbsp; The 'mean' times help to verify that a 'minimum' is not overly-optimistic due to CPU branch prediction.&nbsp; This approach is simple and is not affected by transient system background activity.&nbsp; By following this process, my benchmark results are very consistent -- often equal-to-the-nanosecond!&nbsp; This makes it very easy to know whether a change helps performance or not.</p>
<h2><a id=how-to-perf href="#how-to-perf">Basic Skill #2: How to Profile with <code>perf</code></a></h2>
<p>A profiler tells you where your performance bottlenecks are.&nbsp; Here is a quick tutorial of how I profile with <code>perf</code> on Linux.&nbsp; <a href="#perf-tips"><em>If you already know how to profile your code, you can skip to the Performance Tips section.</em></a></p>
<p>First, write a loop that performs the operation that you are trying to optimize.&nbsp; The longer you run your loop, the better your statistics will be.&nbsp; Here's an example of evaluating an expression with <code>fasteval</code>.&nbsp; (If you are curious or confused about this code, <a href="https://docs.rs/fasteval/#examples">see the <code>fasteval</code> examples</a>.):</p>
<pre><code class="rust">fn main() -&gt; Result&lt;(), fasteval::Error&gt; {
    // 20 million iterations will be long enough for this example.
    for _ in 0..20_000_000i64 {
        // Evaluate a simple expression:
        let val = fasteval::ez_eval(&quot;3^2 + 1&quot;, &amp;mut fasteval::EmptyNamespace)?;

        assert_eq!(val, 10.0);
    }

    Ok(())
}
</code></pre>

<p>Let's see how fast this is:</p>
<pre><code class="bash">user@asus:~/tmp/github.com/fasteval$ cargo build --release
user@asus:~/tmp/github.com/fasteval$ time cargo run --release
real    0m9.187s
user    0m9.183s
sys     0m0.004s
</code></pre>

<p>It took a bit over 9 seconds to run 20 million iterations.&nbsp; It's not bad, but we can do much better.&nbsp; Let's use <code>perf</code> to see where most of the time is spent:</p>
<pre><code class="bash">$ # Pre-Compile so compilation is not included in the profile:
$ cargo build --release

$ # Capture a performance profile:
$ perf record --call-graph dwarf -- cargo run --release

$ # View the profile statistics:
$ perf report


Samples: 1K of event 'cycles:ppp', Event count (approx.): 49289530718
  Children      Self  Command   Shared Object       Symbol
+   88.43%     2.71%  perfdemo  perfdemo            [.] fasteval::ez::ez_eval
+   87.34%     0.34%  perfdemo  perfdemo            [.] perfdemo::main
+   87.34%     0.00%  perfdemo  perfdemo            [.] _start
+   87.34%     0.00%  perfdemo  libc-2.27.so        [.] __libc_start_main
+   87.34%     0.00%  perfdemo  perfdemo            [.] main
+   87.34%     0.00%  perfdemo  perfdemo            [.] std::rt::lang_start_internal
+   87.34%     0.00%  perfdemo  perfdemo            [.] std::panic::catch_unwind (inlined)
+   87.34%     0.00%  perfdemo  perfdemo            [.] std::panicking::try (inlined)
+   87.34%     0.00%  perfdemo  perfdemo            [.] __rust_maybe_catch_panic
+   87.34%     0.00%  perfdemo  perfdemo            [.] std::panicking::try::do_call
+   87.34%     0.00%  perfdemo  perfdemo            [.] std::rt::lang_start_internal::_$u7b$$u7b$closure$u7d$$u7d$::h7508d080ecc0582e (inlined)
+   87.34%     0.00%  perfdemo  perfdemo            [.] std::rt::lang_start::_$u7b$$u7b$closure$u7d$$u7d$::h56279dc72bc5209a
+   31.34%     9.12%  perfdemo  perfdemo            [.] fasteval::parser::Parser::read_expression
+   28.87%     0.00%  perfdemo  libc-2.27.so        [.] __GI___libc_malloc (inlined)
+   23.80%    13.86%  perfdemo  perfdemo            [.] &lt;fasteval::parser::Expression as fasteval::evaler::Evaler&gt;::eval
+   22.10%     6.29%  perfdemo  perfdemo            [.] fasteval::parser::Parser::read_value
+   15.93%    13.37%  perfdemo  libc-2.27.so        [.] _int_malloc
+   13.04%     8.87%  perfdemo  perfdemo            [.] core::num::dec2flt::dec2flt
+   11.77%    11.77%  perfdemo  libc-2.27.so        [.] malloc
+    8.76%     8.76%  perfdemo  [kernel]            [k] 0xffffffff93e018f0
+    8.58%     0.00%  perfdemo  libc-2.27.so        [.] __GI___libc_free (inlined)
+    7.82%     7.82%  perfdemo  libc-2.27.so        [.] cfree@GLIBC_2.2.5
+    5.90%     0.00%  perfdemo  libc-2.27.so        [.] __memcpy_sse2_unaligned_erms (inlined)
+    5.78%     5.78%  perfdemo  libc-2.27.so        [.] __memmove_sse2_unaligned_erms
+    4.83%     1.83%  perfdemo  perfdemo            [.] core::ptr::real_drop_in_place
+    4.15%     4.12%  perfdemo  perfdemo            [.] &lt;f64 as core::num::dec2flt::rawfp::RawFloat&gt;::short_fast_pow10
+    3.88%     0.00%  perfdemo  libc-2.27.so        [.] _int_free (inlined)
+    2.64%     0.00%  perfdemo  [kernel]            [k] 0xb9430a98c55210ff
+    1.95%     0.00%  perfdemo  [unknown]           [.] 0xffffffffffffffff
+    1.86%     0.00%  perfdemo  [unknown]           [.] 0x00007ffeb2f692c7
+    1.84%     0.00%  perfdemo  perfdemo            [.] _fini
+    0.93%     0.90%  perfdemo  perfdemo            [.] fasteval::evaler::&lt;impl fasteval::parser::BinaryOp&gt;::binaryop_eval
+    0.89%     0.88%  perfdemo  perfdemo            [.] core::num::dec2flt::parse::parse_decimal
     0.83%     0.37%  perfdemo  perfdemo            [.] &lt;alloc::vec::Vec&lt;T&gt; as core::ops::drop::Drop&gt;::drop
+    0.80%     0.00%  perfdemo  [unknown]           [.] 0x4023ffffffffffff
+    0.72%     0.72%  perfdemo  perfdemo            [.] __rdl_alloc
+    0.72%     0.00%  perfdemo  perfdemo            [.] std::sys::unix::alloc::&lt;impl core::alloc::GlobalAlloc for std::alloc::System&gt;::alloc (inlined)
+    0.71%     0.71%  perfdemo  perfdemo            [.] core::ptr::real_drop_in_place
+    0.67%     0.04%  perfdemo  libm-2.27.so        [.] __pow
     0.64%     0.64%  perfdemo  perfdemo            [.] core::num::dec2flt::extract_sign
     0.58%     0.25%  perfdemo  libm-2.27.so        [.] __ieee754_pow_sse2
+    0.52%     0.00%  perfdemo  [unknown]           [.] 0x00007ffeb2f68cbf
+    0.50%     0.00%  perfdemo  [unknown]           [.] 0x00007ffeb2f68fe7
     0.33%     0.33%  perfdemo  [kernel]            [k] 0xffffffff93e009a7
     0.30%     0.00%  perfdemo  [unknown]           [.] 0x00007ffeb2f6906f
     0.23%     0.00%  perfdemo  [unknown]           [.] 0x000055ba1c29eb97
     0.21%     0.00%  perfdemo  libc-2.27.so        [.] tcache_get (inlined)
     0.20%     0.00%  perfdemo  [unknown]           [.] 0x0000000000000007
     0.16%     0.15%  perfdemo  perfdemo            [.] core::ptr::real_drop_in_place
     0.15%     0.08%  perfdemo  perfdemo            [.] core::ptr::real_drop_in_place
     0.12%     0.00%  perfdemo  libc-2.27.so        [.] tcache_put (inlined)
</code></pre>

<p>From the above report, I can see that much of the time is spent on memory operations:</p>
<ul>
<li><code>28.87%/??.??%  __GI___libc_malloc (inlined)</code></li>
<li><code>15.93%/13.37%  _int_malloc</code></li>
<li><code>11.77%/11.77%  malloc</code></li>
<li>&nbsp;&nbsp;<code>8.58%/ ?.??%  __GI___libc_free (inlined)</code></li>
<li>&nbsp;&nbsp;<code>7.82%/ 7.82%  cfree@GLIBC_2.2.5</code></li>
<li>&nbsp;&nbsp;<code>5.90%/ ?.??%  __memcpy_sse2_unaligned_erms (inlined)</code></li>
<li>&nbsp;&nbsp;<code>5.78%/ 5.78%  __memmove_sse2_unaligned_erms</code></li>
<li>&nbsp;&nbsp;<code>3.88%/ ?.??%  _int_free (inlined)</code></li>
</ul>
<p><code>fasteval</code> allows you to use a <a href="https://docs.rs/fasteval/slab/index.html"><code>Slab</code></a> -- a pre-allocated block of memory, which can eliminate most of the above memory operations and also allows us to save the parse results so we don't need to repeat the parse in the loop:</p>
<pre><code class="rust">use fasteval::Evaler;  // use this trait so we can call eval().
fn main() -&gt; Result&lt;(), fasteval::Error&gt; {
    // Allocate a block of memory:
    let mut slab = fasteval::Slab::new();

    // Pre-parse the expression, placing it into `slab`:
    let expr_ref = fasteval::parse(&quot;3^2 + 1&quot;, &amp;mut slab.ps)?.from(&amp;slab.ps);

    for _ in 0..20_000_000i64 {
        // Evaluate the pre-parsed expression:
        let val = expr_ref.eval(&amp;slab, &amp;mut fasteval::EmptyNamespace)?;

        assert_eq!(val, 10.0);
    }

    Ok(())
}
</code></pre>

<p>How is the performance now?</p>
<pre><code class="bash">$ time cargo run --release
real    0m1.899s
user    0m1.895s
sys     0m0.004s
</code></pre>

<p>It's getting better -- now it takes less than 2 seconds to run 20 million iterations.&nbsp; Let's do one more profiling pass:</p>
<pre><code class="bash">$ perf record --call-graph dwarf -- cargo run --release
$ perf report


Samples: 258  of event 'cycles:ppp', Event count (approx.): 12442799113
  Children      Self  Command   Shared Object       Symbol
+   91.56%    41.60%  perfdemo  perfdemo            [.] &lt;fasteval::parser::Expression as fasteval::evaler::Evaler&gt;::eval
+   90.00%     0.00%  perfdemo  perfdemo            [.] _start
+   90.00%     0.00%  perfdemo  libc-2.27.so        [.] __libc_start_main
+   90.00%     0.00%  perfdemo  perfdemo            [.] main
+   90.00%     0.00%  perfdemo  perfdemo            [.] std::rt::lang_start_internal
+   90.00%     0.00%  perfdemo  perfdemo            [.] std::panic::catch_unwind (inlined)
+   90.00%     0.00%  perfdemo  perfdemo            [.] std::panicking::try (inlined)
+   90.00%     0.00%  perfdemo  perfdemo            [.] __rust_maybe_catch_panic
+   90.00%     0.00%  perfdemo  perfdemo            [.] std::panicking::try::do_call
+   90.00%     0.00%  perfdemo  perfdemo            [.] std::rt::lang_start_internal::_$u7b$$u7b$closure$u7d$$u7d$::h7508d080ecc0582e (inlined)
+   90.00%     0.00%  perfdemo  perfdemo            [.] std::rt::lang_start::_$u7b$$u7b$closure$u7d$$u7d$::h56279dc72bc5209a
+   90.00%     0.21%  perfdemo  perfdemo            [.] perfdemo::main
+   35.02%     0.00%  perfdemo  libc-2.27.so        [.] __GI___libc_free (inlined)
+   27.18%    27.18%  perfdemo  libc-2.27.so        [.] cfree@GLIBC_2.2.5
+   26.57%     0.00%  perfdemo  libc-2.27.so        [.] _int_free (inlined)
+   17.36%     0.00%  perfdemo  libc-2.27.so        [.] tcache_put (inlined)
+    9.74%     9.74%  perfdemo  [kernel]            [k] 0xffffffff93e018f0
+    7.47%     7.37%  perfdemo  libm-2.27.so        [.] __pow
+    7.27%     7.27%  perfdemo  libm-2.27.so        [.] __ieee754_pow_sse2
+    7.09%     0.00%  perfdemo  [unknown]           [.] 0x4007ffffffffffff
+    4.44%     0.00%  perfdemo  libc-2.27.so        [.] __GI___libc_malloc (inlined)
+    4.43%     4.43%  perfdemo  libc-2.27.so        [.] malloc
+    1.29%     0.00%  perfdemo  [unknown]           [.] 0x00007fff109bcc2f
+    0.98%     0.98%  perfdemo  perfdemo            [.] __rdl_dealloc
+    0.98%     0.00%  perfdemo  perfdemo            [.] std::sys::unix::alloc::&lt;impl core::alloc::GlobalAlloc for std::alloc::System&gt;::dealloc (inlined)
     0.50%     0.00%  perfdemo  [unknown]           [.] 0x000056525f9d4a6f
     0.31%     0.31%  perfdemo  perfdemo            [.] fasteval::evaler::&lt;impl fasteval::parser::BinaryOp&gt;::binaryop_eval
     0.27%     0.00%  perfdemo  [unknown]           [.] 0xffffffffffffffff
     0.26%     0.26%  perfdemo  perfdemo            [.] __rdl_alloc
     0.26%     0.00%  perfdemo  perfdemo            [.] std::sys::unix::alloc::&lt;impl core::alloc::GlobalAlloc for std::alloc::System&gt;::alloc (inlined)
     0.22%     0.22%  perfdemo  libc-2.27.so        [.] __memmove_sse2_unaligned_erms
     0.22%     0.00%  perfdemo  libc-2.27.so        [.] __memcpy_sse2_unaligned_erms (inlined)
     0.21%     0.00%  perfdemo  perfdemo            [.] 0x000056525eaae9df
     0.18%     0.00%  cargo     libc-2.27.so        [.] __libc_start_main
     0.17%     0.00%  cargo     cargo               [.] main
     0.17%     0.00%  cargo     cargo               [.] std::rt::lang_start_internal
     0.17%     0.00%  cargo     cargo               [.] _start
     0.17%     0.00%  cargo     cargo               [.] std::panic::catch_unwind (inlined)
     0.17%     0.00%  cargo     cargo               [.] std::panicking::try (inlined)
     0.17%     0.00%  cargo     cargo               [.] __rust_maybe_catch_panic
     0.17%     0.00%  cargo     cargo               [.] std::panicking::try::do_call
     0.17%     0.00%  cargo     cargo               [.] std::rt::lang_start_internal::_$u7b$$u7b$closure$u7d$$u7d$::h7508d080ecc0582e (inlined)
     0.17%     0.00%  cargo     cargo               [.] std::rt::lang_start::_$u7b$$u7b$closure$u7d$$u7d$::h27e2708c839469d0
     0.17%     0.00%  cargo     cargo               [.] cargo::main
     0.16%     0.00%  cargo     cargo               [.] cargo::ops::registry::needs_custom_http_transport
     0.16%     0.00%  cargo     cargo               [.] cargo::ops::registry::http_proxy
     0.16%     0.00%  cargo     cargo               [.] git2::config::Config::open_default
     0.16%     0.00%  cargo     cargo               [.] libgit2_sys::init
     0.16%     0.00%  cargo     cargo               [.] std::sync::once::Once::call_inner
     0.16%     0.00%  cargo     cargo               [.] std::sync::once::Once::call_once::_$u7b$$u7b$closure$u7d$$u7d$::h879af7ebe2300f84
     0.16%     0.00%  cargo     libpthread-2.27.so  [.] __pthread_once_slow
</code></pre>

<p>Let's focus on this line:</p>
<ul>
<li>91.56%/41.60% <code>&lt;fasteval::parser::Expression as fasteval::evaler::Evaler&gt;::eval</code></li>
</ul>
<p>As expected, most of the time is spent in <code>eval()</code> within the loop.&nbsp; If you know that you will be evaluating an expression many times, you can tell <code>fasteval</code> to compile it into a more efficient form:</p>
<pre><code class="rust">use fasteval::Evaler;    // use this trait so we can call eval().
use fasteval::Compiler;  // use this trait so we can call compile().
fn main() -&gt; Result&lt;(), fasteval::Error&gt; {
    // Allocate a block of memory:
    let mut slab = fasteval::Slab::new();

    // Pre-parse and Compile the expression:
    let compiled = fasteval::parse(&quot;3^2 + 1&quot;, &amp;mut slab.ps)?.from(&amp;slab.ps).compile(&amp;slab.ps, &amp;mut slab.cs);

    for _ in 0..20_000_000i64 {
        // Evaluate the compiled expression:
        let val = fasteval::eval_compiled!(compiled, &amp;slab, &amp;mut fasteval::EmptyNamespace);

        assert_eq!(val, 10.0);
    }

    Ok(())
}
</code></pre>

<p>Let's see the performance:</p>
<pre><code class="bash">$ time cargo run --release
real    0m0.048s
user    0m0.037s
sys     0m0.012s
</code></pre>

<p>20 million iterations in under 50 milliseconds -- a <strong>190x</strong> improvement from where we started.&nbsp; Not too bad!&nbsp; Not too bad at all.</p>
<h2><a id=perf-tips href="#perf-tips">Performance Tip #1: Compile with <code>RUSTFLAGS="--emit=asm"</code></a></h2>
<p>I'm listing this tip first because it's so easy to do (it's just a compilation flag, not a code change), and it can result in a <em>surprising</em> performance boost.&nbsp; By emitting assembly files during compilation, LLVM is able to perform much better optimizations (particularly Variable Localization).</p>
<p>Let's demonstrate this with an example:</p>
<pre><code class="rust">#![feature(test)]
extern crate test;
use test::{Bencher, black_box};

#[bench]
fn emit_asm_demo(ben:&amp;mut Bencher) {
    let (a,b,c) = (1.0f64, 2.0f64, 3.0f64);
    ben.iter(|| {
        for _ in 0..1000000 {
            black_box(a + b + c);
        }
    });
}
</code></pre>

<p>Here's a comparison between normal compilation and compilation with ASM emission:</p>
<pre><code>$ cargo bench
test emit_asm_demo ... bench:     685,107 ns/iter (+/- 99,740)

$ RUSTFLAGS=&quot;--emit=asm&quot; cargo bench
test emit_asm_demo ... bench:     331,837 ns/iter (+/- 57,163)

</code></pre>

<p>...ASM emission makes it run <strong>twice as fast</strong>!&nbsp; It seems like ASM emission helps LLVM do a better job of Variable Localization (putting the data closer to the executing code).&nbsp; Sometimes, we can do this manually, for example:</p>
<pre><code class="rust">#[bench]
fn manual_localization_demo(ben:&amp;mut Bencher) {
    let (a,b,c) = (1.0f64, 2.0f64, 3.0f64);
    ben.iter(|| {
        let (a,b,c) = (a,b,c);  // Manual Variable Localization
        for _ in 0..1000000 {
            black_box(a + b + c);
        }
    });
}
</code></pre>

<p>...and now our code runs at the same speed with or without ASM emission:</p>
<pre><code>$ cargo bench
test manual_localization_demo ... bench:     335,786 ns/iter (+/- 57,439)

$ RUSTFLAGS=&quot;--emit=asm&quot; cargo bench
test manual_localization_demo ... bench:     333,922 ns/iter (+/- 33,784)
</code></pre>

<p>But for more complex situations, the compiler can usually do a better job than a person.&nbsp; That's why I suggest that you always use <code>RUSTFLAGS="--emit=asm"</code> when you compile.</p>
<h2><a id=hidden-costs href="#hidden-costs">Performance Tip #2: Understand Hidden Costs</a></h2>
<p>The reason Rust performance was so unintuitive to me as a beginner was because of all the hidden costs.&nbsp; Many of the standard operations that you're used to from other lanugages actually do more work (safety checks and auto-conversions) in Rust, and of course nothing is free;&nbsp; What you gain in safety or convenience is often paid for in performance.</p>
<h3><a id=indexing-cost href="#indexing-cost">Indexing &amp; Slicing</a></h3>
<p>Bounds Checks</p>
<h3><a id=inline-cost href="#inline-cost">Inlined functions still have overhead</a></h3>
<p>Macros are free</p>
<h3><a id=try-cost href="#try-cost">'?' performs conversions</a></h3>
<p>Return values if you know what you have</p>
<h2><a id=reduce-redundancy href="#reduce-redundancy">Performance Tip #3: Reduce Memory Operations &amp; Redundant Work</a></h2>
<ul>
<li>Slab indexing side-steps borrow-checker but in a mostly-safe way.</li>
<li>Parsing redundancy</li>
</ul>
<h2><a id=comments href="#comments">Comments</a></h2>
<ul>
<li><a href="https://www.reddit.com/r/algotrading/comments/ejbrju/how_many_of_you_are_using_a_topsecret_trading/">Comments on Reddit</a></li>
<li><a href="#todo">Comments on HackerNews</a></li>
</ul></article>

    </section>
    <section id=footer>
      <hr>
      🄯 2020
    </section>
  </body>
</html>




