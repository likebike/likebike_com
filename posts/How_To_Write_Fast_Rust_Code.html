<!DOCTYPE html>
<!--[if lt IE 7]>  <html class="no-js ie ie6 ie-lte9 ie-lte8 ie-lte7 ie-lte6" lang=en> <![endif]-->
<!--[if IE 7]>     <html class="no-js ie ie7 ie-lte9 ie-lte8 ie-lte7" lang=en> <![endif]-->
<!--[if IE 8]>     <html class="no-js ie ie8 ie-lte9 ie-lte8" lang=en> <![endif]-->
<!--[if IE 9]>     <html class="no-js ie ie9 ie-lte9" lang=en> <![endif]-->
<!--[if gt IE 9]>  <html class="no-js ie ie-gt9" lang=en> <![endif]-->
<!--[if !IE]><!--> <html class="no-js not-ie" lang=en> <!--<![endif]-->
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link rel="shortcut icon" type="image/vnd.microsoft.icon" href="/favicon.ico">
    <title>How To Write Fast Rust Code</title>
    <link rel="stylesheet" type="text/css" href="/static/lib/yui-3.12.0/reset_base_fonts-min.css">
    <link rel="stylesheet" type="text/css" href="/static/lib/font-awesome-4.5.0/css/font-awesome.css">
    <link rel="stylesheet" type="text/css" href="/static/css/github-markdown.css?_=1579155942.48">
    <link rel="stylesheet" type="text/css" href="/static/css/base_pretty.css?_=1579165493.41">
    
    <script type="text/javascript" src="/static/lib/underscore-1.5.2/underscore.js"></script>
    <script type="text/javascript" src="/static/lib/jquery-1.10.2/jquery.js"></script>
    <script type="text/javascript">
      jQuery.noConflict();
    </script>
    <script type="text/javascript" src="/static/js/base_pretty.js?_=1569532676.98"></script>
    
  </head>
  <body>
    <section id=header>
      <a id=logo href="/index.html"><div class="asciiLogo vcompressed">
  -------  ___@
-------   `\  L,
  ----  (*)/  (*)
</div>
      <div id=slogan>likebike.com</div></a>
    </section>
    <section id=content>
      



<article class="markdown-body"><h1>How To Write Fast Rust Code</h1>
<p>Author: <a href="http://likebike.com/">Christopher Sebastian</a><br>
Date: 2020-01-17</p>
<h3>Contents</h3>
<ul>
<li><a href="#the-journey">The Journey from <code>eval</code> to <code>fasteval</code></a></li>
<li><a href="#how-to-measure">Basic Skill #1: How To Take Noise-Free Measurements</a></li>
<li><a href="#how-to-perf">Basic Skill #2: How to Profile with <code>perf</code></a></li>
<li><a href="#emit-asm">Performance Tip #1: Compile with <code>RUSTFLAGS="--emit=asm"</code></a></li>
<li><a href="#opt-hot-path">Performance Tip #2: Optimize the Critical Path</a></li>
<li><a href="#reduce-redundancy">Performance Tip #3: Reduce Redundant Work</a></li>
<li><a href="#comments">Comments</a></li>
</ul>
<h2><span id=the-journey>The Journey from <code>eval</code> to <code>fasteval</code></span></h2>
<p>I did a line-for-line port of my <code>eval</code> library from Go to Rust, and right away it was <strong>5x</strong> faster; I was pretty happy.&nbsp; But when I tried to further improve performance using techniques from other languages, it got slower...&nbsp; and the harder I tried, the slower it got!&nbsp; Rust performance was <em>not</em> intuitive to me.</p>
<p>Finally, after learning <em>why</em> my code was slow, I was able to boost performance <strong>12000x</strong>, and my library was worthy of a new name: <a href="https://github.com/likebike/fasteval"><code>fasteval</code></a>.</p>
<p><a href="https://github.com/likebike/fasteval#performance-benchmarks"><img alt="fasteval Performance" src="https://raw.githubusercontent.com/likebike/fasteval/master/benches/results/20191225/fasteval-compiled.png"></a></p>
<p>Here is a log chart showing <a href="https://github.com/likebike/fasteval">fasteval</a>'s performance compared to other similar libraries.&nbsp; <a href="https://github.com/likebike/fasteval">fasteval</a> is represented by the little blue columns, and as you can see, it is <em>significantly</em> faster.</p>
<p>Rust performance makes sense to me now.&nbsp; Here are the lessons I learned.</p>
<h2><span id=how-to-measure>Basic Skill #1: How To Take Noise-Free Measurements</span></h2>
<p><em>ALWAYS PERFORM A <code>release</code> BUILD OF YOUR PROGRAM WHEN MEASURING PERFORMANCE.&nbsp; Measurements from a non-release build will be very misleading.</em></p>
<p>The first step to improving performance is to measure, measure, measure... but these measurements will be affected by <a href="https://easyperf.net/blog/2019/08/02/Perf-measurement-environment-on-Linux">many variables</a>.&nbsp; I try to eliminate three of them: Background Applications, Power Management, and Binary Layout.</p>
<h3><span id=bg-apps>Background Applications</span></h3>
<p>This one's easy: close all the background apps, <em>especially</em> web browsers which constantly consume cycles from all your CPU cores.</p>
<h3><span id=power-mgt>CPU Power Management</span></h3>
<p>Here is how I disable power-saving mode on Ubuntu 18.04:</p>
<pre><code class="bash">for F in /sys/devices/system/cpu/cpufreq/policy*/scaling_governor; do echo performance &gt;$F; done
</code></pre>

<h3><span id=layout-rand>Layout Randomization</span></h3>
<p>The compiler often makes poor decisions about the placement of your code within the binary, and your performance suffers.&nbsp; To mitigate this, I use a Layout Randomization technique similar to <a href="https://www.youtube.com/watch?v=r-TLSBdHe1A">Coz</a>: during each iteration of my benchmark loop, I inject a random number of no-op instructions into my benchmark code (using <code>sed</code>).&nbsp; This shifts everything around in the address space so that I end up hitting all fast and slow scenarios.&nbsp; I then run the benchmark loop many times, until I no longer observe any performance improvements for 500 seconds.&nbsp; (This usually takes around 15 minutes on my system.)&nbsp; At that point, I say that I have reached a stable point and can draw conclusions from the statistics.</p>
<p>I define this macro in my benchmark code:</p>
<pre><code class="rust">#![feature(test)]
extern crate test;
use test::{Bencher, black_box};

macro_rules! memshift {
    () =&gt; { 
        {
            let x = black_box(0);
            let x = black_box(x+1);

            //SHIFT_CODE

            black_box(x);  // Silence 'unused variable' warning.
        }
    }
}
</code></pre>

<p>I then call <code>memshift!();</code> at the beginning of all benchmark functions.</p>
<p>Here is my benchmark loop, which performs Layout Randomization:</p>
<pre><code class="bash">while true; do
    echo &quot;time: $(date +%s)&quot;;
    cat bench.rs.tmpl | sed &quot;s|//SHIFT_CODE|$( N=$(( 1 + $RANDOM % 1024 ));
                                               while [[ $N &gt; 0 ]]; do
                                                   N=$(( $N - 1 ));
                                                   echo -n 'let x=black_box(x+1);';
                                               done )|g&quot; &gt;bench.rs;
    RUSTFLAGS=&quot;--emit=asm&quot; cargo bench;
done &gt;bench.out
</code></pre>

<p>I monitor the results with this:</p>
<pre><code class="bash">cat bench.out | awk -v &quot;now=$(date +%s)&quot; '
    $1==&quot;time:&quot;{when=$2}
    $3==&quot;...&quot; &amp;&amp; $4==&quot;bench:&quot; {
        gsub(/,/, &quot;&quot;, $5);
        v=$5+0;
        counts[$2]+=1; sums[$2]+=v;
        if (mins[$2]==&quot;&quot; || v&lt;mins[$2]) {
            mins[$2]=v; w[$2]=when;
        }
    }
    END{
        printf &quot;%-40s %9s %9s %16s\n&quot;, &quot;000_NAME&quot;, &quot;MEAN&quot;, &quot;MIN&quot;, &quot;WHEN&quot;;
        for (k in mins) {
            printf &quot;%-40s %9d %9d ns/iter    %5ds ago\n&quot;,k,sums[k]/counts[k],mins[k],now-w[k];
        }
    }
' | sort
</code></pre>

<p>I use the 'minimum' times as my final result.&nbsp; The 'mean' times help to verify that a 'minimum' is not overly-optimistic due to CPU branch prediction.&nbsp; This approach is simple and is not affected by transient system background activity.&nbsp; By following this process, my benchmark results are very consistent -- often equal-to-the-nanosecond!&nbsp; This makes it very easy to know whether a change helps performance or not.</p>
<h2><span id=how-to-perf>Basic Skill #2: How to Profile with <code>perf</code></span></h2>
<p>A profiler tells you where your performance bottlenecks are.&nbsp; Here is a quick tutorial of how I profile with <code>perf</code> on Linux.&nbsp; <a href="#emit-asm"><em>If you already know how to profile your code, you can skip to the Performance Tips section.</em></a></p>
<p>First, write a loop that performs the operation that you are trying to measure.&nbsp; The longer you run your loop, the better your statistics will be.&nbsp; Here's an example of evaluating an expression with <a href="https://github.com/likebike/fasteval">fasteval</a>.&nbsp; (If you are curious or confused about this code, <a href="https://docs.rs/fasteval/#examples">see the fasteval examples</a>.):</p>
<pre><code class="rust">fn main() -&gt; Result&lt;(), fasteval::Error&gt; {
    // 20 million iterations will be long enough for this example.
    for _ in 0..20_000_000i64 {
        // Evaluate a simple expression:
        let val = fasteval::ez_eval(&quot;3^2 + 1&quot;, &amp;mut fasteval::EmptyNamespace)?;

        assert_eq!(val, 10.0);
    }

    Ok(())
}
</code></pre>

<p>Let's see how fast this is:</p>
<pre><code class="bash">user@asus:~/tmp/github.com/fasteval$ cargo build --release
user@asus:~/tmp/github.com/fasteval$ time cargo run --release
real    0m9.187s
user    0m9.183s
sys     0m0.004s
</code></pre>

<p>It took a bit over 9 seconds to run 20 million iterations.&nbsp; It's ok, but we can do much better.&nbsp; Let's use <code>perf</code> to see where most of the time is spent:</p>
<pre><code class="bash">$ # Pre-Compile so compilation is not included in the profile:
$ cargo build --release

$ # Capture a performance profile:
$ perf record --call-graph dwarf -- cargo run --release

$ # View the profile statistics:
$ perf report


Samples: 1K of event 'cycles:ppp', Event count (approx.): 49289530718
  Children      Self  Command   Shared Object       Symbol
+   88.43%     2.71%  perfdemo  perfdemo            [.] fasteval::ez::ez_eval
+   87.34%     0.34%  perfdemo  perfdemo            [.] perfdemo::main
+   87.34%     0.00%  perfdemo  perfdemo            [.] _start
+   87.34%     0.00%  perfdemo  libc-2.27.so        [.] __libc_start_main
+   87.34%     0.00%  perfdemo  perfdemo            [.] main
+   87.34%     0.00%  perfdemo  perfdemo            [.] std::rt::lang_start_internal
+   87.34%     0.00%  perfdemo  perfdemo            [.] std::panic::catch_unwind (inlined)
+   87.34%     0.00%  perfdemo  perfdemo            [.] std::panicking::try (inlined)
+   87.34%     0.00%  perfdemo  perfdemo            [.] __rust_maybe_catch_panic
+   87.34%     0.00%  perfdemo  perfdemo            [.] std::panicking::try::do_call
+   87.34%     0.00%  perfdemo  perfdemo            [.] std::rt::lang_start_internal::_$u7b$$u7b$closure$u7d$$u7d$::h7508d080ecc0582e (inlined)
+   87.34%     0.00%  perfdemo  perfdemo            [.] std::rt::lang_start::_$u7b$$u7b$closure$u7d$$u7d$::h56279dc72bc5209a
+   31.34%     9.12%  perfdemo  perfdemo            [.] fasteval::parser::Parser::read_expression
+   28.87%     0.00%  perfdemo  libc-2.27.so        [.] __GI___libc_malloc (inlined)
+   23.80%    13.86%  perfdemo  perfdemo            [.] &lt;fasteval::parser::Expression as fasteval::evaler::Evaler&gt;::eval
+   22.10%     6.29%  perfdemo  perfdemo            [.] fasteval::parser::Parser::read_value
+   15.93%    13.37%  perfdemo  libc-2.27.so        [.] _int_malloc
+   13.04%     8.87%  perfdemo  perfdemo            [.] core::num::dec2flt::dec2flt
+   11.77%    11.77%  perfdemo  libc-2.27.so        [.] malloc
+    8.76%     8.76%  perfdemo  [kernel]            [k] 0xffffffff93e018f0
+    8.58%     0.00%  perfdemo  libc-2.27.so        [.] __GI___libc_free (inlined)
+    7.82%     7.82%  perfdemo  libc-2.27.so        [.] cfree@GLIBC_2.2.5
+    5.90%     0.00%  perfdemo  libc-2.27.so        [.] __memcpy_sse2_unaligned_erms (inlined)
+    5.78%     5.78%  perfdemo  libc-2.27.so        [.] __memmove_sse2_unaligned_erms
+    4.83%     1.83%  perfdemo  perfdemo            [.] core::ptr::real_drop_in_place
+    4.15%     4.12%  perfdemo  perfdemo            [.] &lt;f64 as core::num::dec2flt::rawfp::RawFloat&gt;::short_fast_pow10
+    3.88%     0.00%  perfdemo  libc-2.27.so        [.] _int_free (inlined)
+    2.64%     0.00%  perfdemo  [kernel]            [k] 0xb9430a98c55210ff
+    1.95%     0.00%  perfdemo  [unknown]           [.] 0xffffffffffffffff
+    1.86%     0.00%  perfdemo  [unknown]           [.] 0x00007ffeb2f692c7
+    1.84%     0.00%  perfdemo  perfdemo            [.] _fini
+    0.93%     0.90%  perfdemo  perfdemo            [.] fasteval::evaler::&lt;impl fasteval::parser::BinaryOp&gt;::binaryop_eval
+    0.89%     0.88%  perfdemo  perfdemo            [.] core::num::dec2flt::parse::parse_decimal
     0.83%     0.37%  perfdemo  perfdemo            [.] &lt;alloc::vec::Vec&lt;T&gt; as core::ops::drop::Drop&gt;::drop
+    0.80%     0.00%  perfdemo  [unknown]           [.] 0x4023ffffffffffff
+    0.72%     0.72%  perfdemo  perfdemo            [.] __rdl_alloc
+    0.72%     0.00%  perfdemo  perfdemo            [.] std::sys::unix::alloc::&lt;impl core::alloc::GlobalAlloc for std::alloc::System&gt;::alloc (inlined)
+    0.71%     0.71%  perfdemo  perfdemo            [.] core::ptr::real_drop_in_place
+    0.67%     0.04%  perfdemo  libm-2.27.so        [.] __pow
     0.64%     0.64%  perfdemo  perfdemo            [.] core::num::dec2flt::extract_sign
     0.58%     0.25%  perfdemo  libm-2.27.so        [.] __ieee754_pow_sse2
+    0.52%     0.00%  perfdemo  [unknown]           [.] 0x00007ffeb2f68cbf
+    0.50%     0.00%  perfdemo  [unknown]           [.] 0x00007ffeb2f68fe7
     0.33%     0.33%  perfdemo  [kernel]            [k] 0xffffffff93e009a7
     0.30%     0.00%  perfdemo  [unknown]           [.] 0x00007ffeb2f6906f
     0.23%     0.00%  perfdemo  [unknown]           [.] 0x000055ba1c29eb97
     0.21%     0.00%  perfdemo  libc-2.27.so        [.] tcache_get (inlined)
     0.20%     0.00%  perfdemo  [unknown]           [.] 0x0000000000000007
     0.16%     0.15%  perfdemo  perfdemo            [.] core::ptr::real_drop_in_place
     0.15%     0.08%  perfdemo  perfdemo            [.] core::ptr::real_drop_in_place
     0.12%     0.00%  perfdemo  libc-2.27.so        [.] tcache_put (inlined)
</code></pre>

<p>From the above report, I can see that much of the time is spent on memory operations:</p>
<ul>
<li><code>28.87%/??.??%  __GI___libc_malloc (inlined)</code></li>
<li><code>15.93%/13.37%  _int_malloc</code></li>
<li><code>11.77%/11.77%  malloc</code></li>
<li>&nbsp;&nbsp;<code>8.58%/ ?.??%  __GI___libc_free (inlined)</code></li>
<li>&nbsp;&nbsp;<code>7.82%/ 7.82%  cfree@GLIBC_2.2.5</code></li>
<li>&nbsp;&nbsp;<code>5.90%/ ?.??%  __memcpy_sse2_unaligned_erms (inlined)</code></li>
<li>&nbsp;&nbsp;<code>5.78%/ 5.78%  __memmove_sse2_unaligned_erms</code></li>
<li>&nbsp;&nbsp;<code>3.88%/ ?.??%  _int_free (inlined)</code></li>
</ul>
<p><a href="https://github.com/likebike/fasteval">fasteval</a> allows you to use a <a href="https://docs.rs/fasteval/latest/fasteval/slab/index.html"><code>Slab</code></a> -- a pre-allocated block of memory, which can eliminate most of the above memory operations and also allows us to save the parse results so we don't need to repeat the parse in the loop:</p>
<pre><code class="rust">use fasteval::Evaler;  // use this trait so we can call eval().
fn main() -&gt; Result&lt;(), fasteval::Error&gt; {
    // Allocate a block of memory:
    let mut slab = fasteval::Slab::new();

    // Pre-parse the expression, placing it into `slab`:
    let parser = fasteval::Parser::new();
    let expr_ref = parser.parse(&quot;3^2 + 1&quot;, &amp;mut slab.ps)?.from(&amp;slab.ps);

    for _ in 0..20_000_000i64 {
        // Evaluate the pre-parsed expression:
        let val = expr_ref.eval(&amp;slab, &amp;mut fasteval::EmptyNamespace)?;

        assert_eq!(val, 10.0);
    }

    Ok(())
}
</code></pre>

<p>How is the performance now?</p>
<pre><code class="bash">$ time cargo run --release
real    0m1.899s
user    0m1.895s
sys     0m0.004s
</code></pre>

<p>It's getting better -- now it takes less than 2 seconds to run 20 million iterations.&nbsp; Let's do one more profiling pass:</p>
<pre><code class="bash">$ perf record --call-graph dwarf -- cargo run --release
$ perf report


Samples: 258  of event 'cycles:ppp', Event count (approx.): 12442799113
  Children      Self  Command   Shared Object       Symbol
+   91.56%    41.60%  perfdemo  perfdemo            [.] &lt;fasteval::parser::Expression as fasteval::evaler::Evaler&gt;::eval
+   90.00%     0.00%  perfdemo  perfdemo            [.] _start
+   90.00%     0.00%  perfdemo  libc-2.27.so        [.] __libc_start_main
+   90.00%     0.00%  perfdemo  perfdemo            [.] main
+   90.00%     0.00%  perfdemo  perfdemo            [.] std::rt::lang_start_internal
+   90.00%     0.00%  perfdemo  perfdemo            [.] std::panic::catch_unwind (inlined)
+   90.00%     0.00%  perfdemo  perfdemo            [.] std::panicking::try (inlined)
+   90.00%     0.00%  perfdemo  perfdemo            [.] __rust_maybe_catch_panic
+   90.00%     0.00%  perfdemo  perfdemo            [.] std::panicking::try::do_call
+   90.00%     0.00%  perfdemo  perfdemo            [.] std::rt::lang_start_internal::_$u7b$$u7b$closure$u7d$$u7d$::h7508d080ecc0582e (inlined)
+   90.00%     0.00%  perfdemo  perfdemo            [.] std::rt::lang_start::_$u7b$$u7b$closure$u7d$$u7d$::h56279dc72bc5209a
+   90.00%     0.21%  perfdemo  perfdemo            [.] perfdemo::main
+   35.02%     0.00%  perfdemo  libc-2.27.so        [.] __GI___libc_free (inlined)
+   27.18%    27.18%  perfdemo  libc-2.27.so        [.] cfree@GLIBC_2.2.5
+   26.57%     0.00%  perfdemo  libc-2.27.so        [.] _int_free (inlined)
+   17.36%     0.00%  perfdemo  libc-2.27.so        [.] tcache_put (inlined)
+    9.74%     9.74%  perfdemo  [kernel]            [k] 0xffffffff93e018f0
+    7.47%     7.37%  perfdemo  libm-2.27.so        [.] __pow
+    7.27%     7.27%  perfdemo  libm-2.27.so        [.] __ieee754_pow_sse2
+    7.09%     0.00%  perfdemo  [unknown]           [.] 0x4007ffffffffffff
+    4.44%     0.00%  perfdemo  libc-2.27.so        [.] __GI___libc_malloc (inlined)
+    4.43%     4.43%  perfdemo  libc-2.27.so        [.] malloc
+    1.29%     0.00%  perfdemo  [unknown]           [.] 0x00007fff109bcc2f
+    0.98%     0.98%  perfdemo  perfdemo            [.] __rdl_dealloc
+    0.98%     0.00%  perfdemo  perfdemo            [.] std::sys::unix::alloc::&lt;impl core::alloc::GlobalAlloc for std::alloc::System&gt;::dealloc (inlined)
     0.50%     0.00%  perfdemo  [unknown]           [.] 0x000056525f9d4a6f
     0.31%     0.31%  perfdemo  perfdemo            [.] fasteval::evaler::&lt;impl fasteval::parser::BinaryOp&gt;::binaryop_eval
     0.27%     0.00%  perfdemo  [unknown]           [.] 0xffffffffffffffff
     0.26%     0.26%  perfdemo  perfdemo            [.] __rdl_alloc
     0.26%     0.00%  perfdemo  perfdemo            [.] std::sys::unix::alloc::&lt;impl core::alloc::GlobalAlloc for std::alloc::System&gt;::alloc (inlined)
     0.22%     0.22%  perfdemo  libc-2.27.so        [.] __memmove_sse2_unaligned_erms
     0.22%     0.00%  perfdemo  libc-2.27.so        [.] __memcpy_sse2_unaligned_erms (inlined)
     0.21%     0.00%  perfdemo  perfdemo            [.] 0x000056525eaae9df
     0.18%     0.00%  cargo     libc-2.27.so        [.] __libc_start_main
     0.17%     0.00%  cargo     cargo               [.] main
     0.17%     0.00%  cargo     cargo               [.] std::rt::lang_start_internal
     0.17%     0.00%  cargo     cargo               [.] _start
     0.17%     0.00%  cargo     cargo               [.] std::panic::catch_unwind (inlined)
     0.17%     0.00%  cargo     cargo               [.] std::panicking::try (inlined)
     0.17%     0.00%  cargo     cargo               [.] __rust_maybe_catch_panic
     0.17%     0.00%  cargo     cargo               [.] std::panicking::try::do_call
     0.17%     0.00%  cargo     cargo               [.] std::rt::lang_start_internal::_$u7b$$u7b$closure$u7d$$u7d$::h7508d080ecc0582e (inlined)
     0.17%     0.00%  cargo     cargo               [.] std::rt::lang_start::_$u7b$$u7b$closure$u7d$$u7d$::h27e2708c839469d0
     0.17%     0.00%  cargo     cargo               [.] cargo::main
     0.16%     0.00%  cargo     cargo               [.] cargo::ops::registry::needs_custom_http_transport
     0.16%     0.00%  cargo     cargo               [.] cargo::ops::registry::http_proxy
     0.16%     0.00%  cargo     cargo               [.] git2::config::Config::open_default
     0.16%     0.00%  cargo     cargo               [.] libgit2_sys::init
     0.16%     0.00%  cargo     cargo               [.] std::sync::once::Once::call_inner
     0.16%     0.00%  cargo     cargo               [.] std::sync::once::Once::call_once::_$u7b$$u7b$closure$u7d$$u7d$::h879af7ebe2300f84
     0.16%     0.00%  cargo     libpthread-2.27.so  [.] __pthread_once_slow
</code></pre>

<p>Let's focus on this line:</p>
<ul>
<li><code>91.56%/41.60%  &lt;fasteval::parser::Expression as fasteval::evaler::Evaler&gt;::eval</code></li>
</ul>
<p>As expected, most of the time is spent in <code>eval()</code> within the loop.&nbsp; If you know that you will be evaluating an expression many times, you can tell <a href="https://github.com/likebike/fasteval">fasteval</a> to compile it into a more efficient form:</p>
<pre><code class="rust">use fasteval::Evaler;    // use this trait so we can call eval().
use fasteval::Compiler;  // use this trait so we can call compile().
fn main() -&gt; Result&lt;(), fasteval::Error&gt; {
    // Allocate a block of memory:
    let mut slab = fasteval::Slab::new();

    // Pre-parse and Compile the expression:
    let parser = fasteval::Parser::new();
    let compiled = parser.parse(&quot;3^2 + 1&quot;, &amp;mut slab.ps)?.from(&amp;slab.ps).compile(&amp;slab.ps, &amp;mut slab.cs);

    for _ in 0..20_000_000i64 {
        // Evaluate the compiled expression:
        let val = fasteval::eval_compiled!(compiled, &amp;slab, &amp;mut fasteval::EmptyNamespace);

        assert_eq!(val, 10.0);
    }

    Ok(())
}
</code></pre>

<p>Let's see the performance:</p>
<pre><code class="bash">$ time cargo run --release
real    0m0.048s
user    0m0.037s
sys     0m0.012s
</code></pre>

<p>20 million iterations in under 50 milliseconds -- a <strong>190x</strong> improvement from where we started.&nbsp; Not bad!</p>
<h2><span id=emit-asm>Performance Tip #1: Compile with <code>RUSTFLAGS="--emit=asm"</code></span></h2>
<p>I'm listing this tip first because it's so easy to do (it's just a compilation flag, not a code change), and because it can result in a <em>surprising</em> performance boost.&nbsp; By emitting assembly files during compilation, LLVM is able to perform much better optimizations (particularly Variable Localization).</p>
<p>Let's demonstrate this with an example:</p>
<pre><code class="rust">#![feature(test)]
extern crate test;
use test::{Bencher, black_box};

#[bench]
fn emit_asm_demo(ben:&amp;mut Bencher) {
    let (a,b,c) = (1.0f64, 2.0f64, 3.0f64);
    ben.iter(|| {
        for _ in 0..1000000 {
            black_box(a + b + c);
        }
    });
}
</code></pre>

<p>Here's a comparison between normal compilation and compilation with ASM emission:</p>
<pre><code>$ cargo bench
test emit_asm_demo ... bench:     685,107 ns/iter (+/- 99,740)

$ RUSTFLAGS=&quot;--emit=asm&quot; cargo bench
test emit_asm_demo ... bench:     331,837 ns/iter (+/- 57,163)

</code></pre>

<p>...ASM emission makes it run <strong>twice as fast</strong>!&nbsp; It seems like ASM emission helps LLVM do a better job of Variable Localization (putting the data closer to the executing code).&nbsp; Sometimes, we can do this manually, for example:</p>
<pre><code class="rust">#[bench]
fn manual_localization_demo(ben:&amp;mut Bencher) {
    let (a,b,c) = (1.0f64, 2.0f64, 3.0f64);
    ben.iter(|| {
        let (a,b,c) = (a,b,c);  // Manual Variable Localization
        for _ in 0..1000000 {
            black_box(a + b + c);
        }
    });
}
</code></pre>

<p>...and now our code runs at the same speed with or without ASM emission:</p>
<pre><code>$ cargo bench
test manual_localization_demo ... bench:     335,786 ns/iter (+/- 57,439)

$ RUSTFLAGS=&quot;--emit=asm&quot; cargo bench
test manual_localization_demo ... bench:     333,922 ns/iter (+/- 33,784)
</code></pre>

<p>But for more complex situations, the compiler can usually do a better job than a person.</p>
<p>The benefits of <code>RUSTFLAGS="--emit=asm"</code> will vary, depending on your platform and code.&nbsp; Sometimes it will help performance, and other times it will hurt performance.&nbsp; You just need to try it and <em>measure</em> the results.</p>
<h2><span id=opt-hot-path>Performance Tip #2: Optimize the Critical Path</span></h2>
<p>I'll say this again, since it's so important: <em>ALWAYS PERFORM A <code>release</code> BUILD OF YOUR PROGRAM WHEN MEASURING PERFORMANCE.&nbsp; Measurements from a non-release build will be very misleading.</em></p>
<p>After you use a profiler to determine the "hot spots" in your code, you can gain a minor performance boost by applying some manual optimizations:</p>
<ul>
<li>
<p><strong>inline <code>pub</code>:</strong>&nbsp; This is basic, but worth mentioning: If you want 'pub' functions to be inlinable across crates, you must mark them with <code>#[inline]</code>.</p>
</li>
<li>
<p><strong>panics:</strong>&nbsp; The compiler usually does a good job of optimizing panicky code, but sometimes panics can bog down your performance.&nbsp; Measure any code that can panic, and test out non-panicky alternatives.</p>
</li>
<li>
<p><strong>indexing:</strong>&nbsp; When you use the index operator (<code>a[i]</code>) on an array/slice/Vec/etc., bounds checks are performed, and a panic can be raised.&nbsp; For most situations, <code>if let</code> <strong>+</strong> <code>get()</code> tends to outperform other indexing techniques (including the <code>unsafe</code> ones!) because it cannot panic and has excellent compiler optimization:</p>
<pre><code>Original:
    let v = a[i];
    ...

Usually Faster:
    if let Some(v) = a.get(i) {
        ...
    }
</code></pre>
</li>
</ul>
<h2><span id=reduce-redundancy>Performance Tip #3: Reduce Redundant Work</span></h2>
<ul>
<li>
<p><strong><span id=reduce-redundancy-mem>Memory:</span></strong>&nbsp; Memory operations are very slow and should be avoided if possible.&nbsp; Rust lifetimes and references help you to reduce memory operations by sharing data, rather than copying.&nbsp; <a href="https://github.com/likebike/fasteval">fasteval</a> also uses a <a href="https://docs.rs/fasteval/latest/fasteval/slab/index.html"><code>Slab</code></a> -- a pre-allocated block of memory -- so it only needs to perform one big allocation instead of many little allocations.&nbsp; Also, the <code>Slab</code> can be re-used, so subsequent evaluations don't need to allocate <em>anything</em>.</p>
</li>
<li>
<p><strong><span id=reduce-redundancy-logic>Logic:</span></strong>&nbsp; It is often possible to improve the efficiency of your logic so that you can accomplish the same goal while doing less work.&nbsp; One example of this situation arises when parsing a string into an <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">AST</a>.&nbsp; The standard process goes something like this:</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Step 1: Use a "tokenizer" to split the string into a list of tokens.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Step 2: Interpret the list of tokens to produce a list of AST nodes.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Step 3: Discard the list of tokens.</p>
<p>This structure is nice because it enables separation of the "tokenization" and "interpretation" logic.&nbsp; The down-side to this structure is the extra work that must be done at run-time: allocating and discarding the temporary token list, writing-and-reading the tokens to-and-from the list, and sharing the list between different parts of the program.&nbsp; <a href="https://github.com/likebike/fasteval">fasteval</a> uses a specialized parser that is able to generate an AST directly from a string, without using tokens.&nbsp; This produces the same result in less than half the time.</p>
</li>
<li>
<p><strong><span id=reduce-redundancy-data>Data:</span></strong>&nbsp; When designing your data structures, try to make them <em>"infallible"</em> -- i.e. design them so that it is <em>impossible</em> to represent an invalid value.&nbsp; If you know that a value will always be valid, you don't need to perform validity checks at runtime.</p>
<p>Here is an easy-to-understand example from <a href="https://github.com/likebike/fasteval">fasteval</a>: the AST node representing a <code>min(...)</code> variadic function call.&nbsp; Example usage is <code>min(3, 2.25, x, -4)</code> or even <code>min(3)</code>, but <code>min()</code> with no args is not valid.&nbsp; Originally, I designed the data structure like this:</p>
<pre><code>struct MinFunc {
    args: Vec&lt;f64&gt;,
}
</code></pre>
<p>...but that structure is <em>fallible</em> -- args could be empty, and you can't take the minimum of nothing!&nbsp; Even though a zero-arg <code>min()</code> call wouldn't make it past the 'parse' phase, I still had to do validity checks in the 'compile' and 'eval' code to defend against incorrect API usage.&nbsp; The 'eval' phase is in the critical path, so these validity checks really add up and hurt performance.</p>
<p>I finally changed the data structure to this:</p>
<pre><code>struct MinFunc {
    first: f64,
    rest : Vec&lt;f64&gt;,
}
</code></pre>
<p>This data structure is <em>infallible</em> -- it is impossible to create a value that represents a no-arg <code>min()</code> call because we are guaranteed to have at least a <code>first</code> value.&nbsp; This allowed me to remove all of those validity checks, which improved performance <em>and</em> safety.</p>
</li>
</ul>
<h2><span id=comments>Comments</span></h2></article>

    </section>
    <section id=footer>
      ðŸ„¯ 2020
    </section>
  </body>
</html>




